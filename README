Correlation Code - v.0.9.1
Dolence & Brunner
Brief summary of what the code does:

The Two-Point Angular Correlation function, simply put, quantifies the excess probability above a random distribution of finding an astronomical body (in this case, galaxy) within a specified angle of another astronomical body. To see how it's applied to a particular data set, see http://arxiv.org/pdf/1303.2432v2.pdf (where the Two-Point function is applied to the seventh data release of the Sloan Digital Sky Survey). If you look at correlate.c, there's two main functions that do the work, dualTreeACSpatial and dualTreeACAngular (to get a feel of how the two algorithms are implemented, refer to http://www.linuxclustersinstitute.org/conferences/archive/2008/PDF/Dolence_98279.pdf). 

The Probabilistic TPACF is an extension of the regular TPACF that incorporates probability weights for individual points when calculating bin counts to calculate the correlation function. 

Methodology: Enable reading in probabilities from source data files, struct containing information about each data point populated with probability values, recursive KD-tree algorithm to traverse the tree and propogate probabilities of points through the tree (description of tree structure in Moore et al.) The main correlation function is then modified to take into account the probability of two points when populating bins. 

Format for data file for precomputation (examples in test/probability_testing/data):

/*RA DEC PROBABILITY*/
170 -1 0.58
169 0.5 0.34
.. .. ..

Instructions for trying out the code on your machine:

***********************
***** 1. Building *****
***********************

Change the "make.config" file to reflect the compiler(s) and flags appropriate to your machine and "params.h" to set the build options.

Type "make all" to run the code. 

If the build was successful, the following files should be located in the "bin" directory:

precompute
correlate
estimate

To build specific programs, you need to run "make" with these targets for separate functions:

			1. pre for Precompute code
			2. corr for Correlation code
			3. est for Estimation code

To clean the build, type "make clean"


***********************
***** 2. Testing ******
***********************

After compiling the code, it's important to verify that the code is functioning.  Test data and correct outputs can be found in the test directory.  If the code has been compiled to run serially, simply enter the test directory and run the "runserial.sh" script using the command "sh runserial.sh" on your system.  This script automates the precomputing and bin counting phases described below and checks the results against the correct answers, reporting if any problems were detected.  If the code is to be run in parallel, in particular through a batch queue, you'll need to run the code appropriately for your system, then use the "compare.sh" script which simply checks the output of the parallel run with the correct answers and reports any problems.


***********************
***** 3. Usage ********
***********************

Computing a correlation involves 3 steps: precomputing, counting pairs, and estimating the correlation from those counts.


**** Precomputing *****

Source data files should be plain text and stored as "RA DEC [z ...]", one source per line.
A file should be created that lists each data file, one per line (just use ls data* > filelist).  For an autocorrelation, the source file should be listed first followed by the randoms.  

Now run precompute with the file list, angular/spatial flag (0 for angular, 1 for spatial), and, optionally, the number of jackknife samples (default set in "params.h") as command line arguments.

Ex.	./precompute filelist 0 32

for an angular calculation, or

Ex.	./precompute filelist 1 16

for a spatial calculation.

Depending on the build parameters in params.h and the size of the data, 2 or more files should be created for each data file.  One (or more) should have .bin and likewise for .tree.


**** Getting bin counts ******

Create a parameter file for the correlation run using the example in the samples directory.  See Dolence & Brunner (2008) for details on the work level parameter.

To run the code, be sure that the working directory of the program contains the *bin and *tree files generated from precomputing, the parameter file which is the command line argument, and the file list used in precomputing.

SERIAL:
	./correlate params.file

PARALLEL WITH OPENMP:
	After building the code with OpenMP enabled (both in params.h and make.config), set the environment variable OMP_NUM_THREADS to the desired number of processes then run the code as if it were serial.  This may be different if your system has a batch queue.

PARALLEL WITH MPI:
	Follow the instructions for your system.

PARALLEL WITH MPI/OPENMP:
	See above.

Once complete, there should be files ending in *bins which contain the unnormalized bin counts and information about the calculation required for estimating the correlation function.


**** Estimation *****

An estimator hasn't been implemented in our code. But you could potentially implement your own Landy-Szalay estimator using http://adsabs.harvard.edu/full/1993ApJ...412...64L as inspiration. 


****************************************************************************

From raw data to the final correlation estimation, the process is as follows:

(1) In the directory where the data resides, generate a file that lists the names of the data files, one per line.  The "source" file should be first in the list, followed by the targets being compared against.

(2) Run the precompute code with the file list and angular/spatial flag as a command line arguments, as well as the desired number of jackknife samples if different from the default.  If no jackknife sample number is supplied, the code defaults to 16 as set in "params.h".  NOTE:  the number of jackknife samples MUST be a power of 2.

(3) Create a parameter file for the correlation code.  Samples can be found in the "samples" directory.

(4) Run the correlation code with the parameter file as a command line argument.

(5) Create a parameter file for the estimation code.  Again, a sample can be found in the "samples"  directory.

(6) Run the estimation code with the parameter file as a command line argument.

DONE!
